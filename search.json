[
  {
    "objectID": "Tensorflow/compiling.html",
    "href": "Tensorflow/compiling.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This module takes a trained Keras neural network and compiles it to fit a MCU. The compiling is done in two parts: first the model is compiled into a Tensorflow Lite model and then it is converted into a C-array, that can be used by Arduino Nano\n\nsource\n\nplot_size\n\n plot_size (model_path)\n\nPlots the size difference before and after quantization Args: model_path: Path to model files Returns: pandas dataframe: Pandas dataframe containing information\n\nsource\n\n\nconvert_model_to_cc\n\n convert_model_to_cc (model_path:str)\n\nCreates model.cc from model.tflite in folder model_path\n\nsource\n\n\nconvert_to_c_array\n\n convert_to_c_array (bytes)\n\nC array conversion\n\nsource\n\n\nconvert_model\n\n convert_model (model_path:str, output_path:str, dataset_path:str,\n                model_params:dict, model_name:str)\n\nModel conversion into TFLite model Args:"
  },
  {
    "objectID": "Documentation/demonstration.html",
    "href": "Documentation/demonstration.html",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "",
    "text": "This document will demonstrate the steps in TinyMLaaS app. Some pages have dependencies on other pages, so go through pages from top to bottom is recommended at the start.\nIn order to run TinyMLaaS end-to-end following components need to be run: - The Frontend - The Backend - The Relay\nThese can all easily be started with the help of docker using the docker-compose-with-bridge.yml file in the Main repository. The application can be started with"
  },
  {
    "objectID": "Documentation/demonstration.html#device",
    "href": "Documentation/demonstration.html#device",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Device",
    "text": "Device\nIn order to install a model to a embedded device, the briging device of the wanted device and the device need to be selected.\nThe first thing that should be done on the Device page is to either add a new bridge or selecting an existing bridge. Let’s add the bridge that was started by docker compose. To do that, add the name of the bridge docker container with the port 8080 as the bridges address. The bridge will also not use a HTTPS connection in this case.\n\n\n\nAdd new bridge\n\n\nAfter adding the bridge, select the wanted bridge by clicking the Select bridge button next to the wanted bridge\n\n\n\nSelect the bridge\n\n\nSelecting a device to which to install the trained machine learning model later on is required. If the wanted device has not been registered already, register it either manually or by selecting it from the list of devices connected to the bridge. Lets add a device connected to the bridge by pressing the Register this device button next to that device\n\n\n\nRegister the new device\n\n\nAdd the missing information on the form and click add\n\n\n\nDevice form\n\n\nThe added device will automatically be selected as the active device.\n\n\n\nSelected device"
  },
  {
    "objectID": "Documentation/demonstration.html#data",
    "href": "Documentation/demonstration.html#data",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Data",
    "text": "Data\nIn order to train a model, a dataset with which to train the model needs to be selected.\n\n\n\nDataset selection complete\n\n\nUser can add images from local storage to selected dataset.\nIf the existing datasets are not enough, a new dataset can be added to the software."
  },
  {
    "objectID": "Documentation/demonstration.html#model",
    "href": "Documentation/demonstration.html#model",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Model",
    "text": "Model\nThis page shows already trained models as well as allows training of a new model.\nLet’s train a new model. For this, we first need to decide the parameters with which to train the model with. This time we chose to train the model with 27 epochs and with a batch size of 56. The image size is 96x96, as this model is trained for an Arduino, which takes pictures of this size.\n\n\n\nTrain a new model\n\n\nAfter the training is done, the software will show an image of the statistics of the training process as well as a test image with a prediction that the newly trained model gave for that picture.\n\n\n\nAfter training"
  },
  {
    "objectID": "Documentation/demonstration.html#compiling",
    "href": "Documentation/demonstration.html#compiling",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Compiling",
    "text": "Compiling\nThe page is responsible for ML compilation. It will turn the selected ML model and turn it into a tflite model as well as generate a C-array of it. The C array is the tflite model turned into bytes stored in a C array, which is required for embedded devices, which do not have a filesystem.\nAfter the compiling is done, the newly compiled model will be selected as the active model.\n\n\n\nCompilation done"
  },
  {
    "objectID": "Documentation/demonstration.html#installing",
    "href": "Documentation/demonstration.html#installing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Installing",
    "text": "Installing\nNow that a model has been compiled, it can be installed on the device that was selected on the Device page. The page shows a single button, install. When this is pressed, the software will install the selected compiled model to the selected device on the selected bridge.\nBe sure that the software has access to the device. If you are not sure, the next command will give all users permissions to read, write and execute to the machine\nchmod 777 /path/to/port\nThis time, the device is connected to /dev/ttyACM0, so it was given permissions.\nNow, install the model to the device.\n\n\n\nInstall successfully complete"
  },
  {
    "objectID": "Documentation/demonstration.html#observing",
    "href": "Documentation/demonstration.html#observing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Observing",
    "text": "Observing\nOn the observing page, user can see real-time predictions from device when the start button has been activated.\n\n\n\nReal-time predictions as device output"
  },
  {
    "objectID": "Documentation/technologies.html",
    "href": "Documentation/technologies.html",
    "title": "Technological choices",
    "section": "",
    "text": "This document is meant to answer why certain technological choices have been made and why certain frameworks have been chosen."
  },
  {
    "objectID": "Documentation/technologies.html#main",
    "href": "Documentation/technologies.html#main",
    "title": "Technological choices",
    "section": "Main",
    "text": "Main\nThe TinyMLaaS-main -repository is meant for mostly building the whole software with docker and documentation. However, some development is also done in this repository. All tensorflow modules are coded here in Jupyter notebooks.\n\nJupyter notebooks and NBDEV\nAs jupyter notebooks can not be used as modules in python, they need to be exported into python modules. This is done with NBDEV. Nbdev also automatically creates documentation from the jupyter notebooks and deploys them to Github pages\n\n\nDocker\nRunning the software is meant to be done with docker. Docker allows running the software on different computers, without the software being platform spesific. Also, all the dependencies required for the software do not need to be installed on the host, rather, they will all be installed in the seperate docker container. If you are not familiar with docker, check out University of Helsinkis course Devops with Docker materials to get a basic understanding of docker.\n\nSysbox\nThere are parts of the software that require starting their own docker containers. For example, the relay will start containers to compile arduino sketches and to install them to the devices. When running the relay itself inside a docker container, there are a few ways of starting a new docker container from this docker container. First, is by using so called sibling containers. This gives the docker container access to the host machines docker socket, which allows it to control other containers on the host machine. However, this has a big security flaw, as if someone gets access to this docker container, they will be able to control all other docker containers on the host machine and start their own containers. The other way is by using docker inside docker, which allows docker containers to be started inside the docker container recursively. This approach requires privileged mode to be set for the docker container, meaning that it will have root privileges on the host machine. In order to run docker machines without privileged mode, Sysbox runtime is used. This allows starting docker containers inside the docker containers without having the docker container in privileged mode, which is a lot more secure and allows for better isolation of the docker containers."
  },
  {
    "objectID": "Documentation/technologies.html#frontend",
    "href": "Documentation/technologies.html#frontend",
    "title": "Technological choices",
    "section": "Frontend",
    "text": "Frontend\n\nStreamlit\nThe frontend of the software is build with Streamlit. This is done to make the development process faster, as this frontend is mainly meant for demo purposes. Streamlit makes it easy to create good looking websites, however, there isn’t much room for cutomization and some features can be quite difficult to create.\nThere is still a dependecy on usbutils. This will be talked more about in the Bridge-section"
  },
  {
    "objectID": "Documentation/technologies.html#backend",
    "href": "Documentation/technologies.html#backend",
    "title": "Technological choices",
    "section": "Backend",
    "text": "Backend\nThe backend is the heart of the software. It does all the communication between all the other modules and does a lot of the heavy lifting of the software. It is created as a API.\n\nFastAPI\nThe backend is created with FastAPI. FastAPI is very powerful for creating API:s, as it has great data validation with the help of Pydantic, it automatically creates good documentation about the different API requests and is simple to understand. To checkout more, read the Starting documentation.\nWhen deploying the API to production, the api will most likely be behind a proxy with some URL that has prefixes. For example, it might be deployed to example.uri.com/api/. For the API to function correctly, the root-path of the API needs to be declared for FastAPI, in this case, /api/.\n\n\nSQLAlchemy\nThe backend talks with the database with sqlalchemy. This means that it is able to talk with any SQL-database without any changes to the backends software.\nAs of now, the database in use is sqlite. However, this is meant to be more of a temporary solution to make development easier. For more information, checkout the suggestions in Suggestions for further development"
  },
  {
    "objectID": "Documentation/technologies.html#bridge",
    "href": "Documentation/technologies.html#bridge",
    "title": "Technological choices",
    "section": "Bridge",
    "text": "Bridge\nThe relay is the part of the software to which the microcontrollers are connected to. It is also done in API style.\n\nFlask\nUnlike the backend, the bridge is created with Flask. Flask is lightweight and easy to understand, which makes sence for the bridge, as the hardware, on which the bridge runs, might not be that powerfull.\n\n\nUsbutils\nTo find USB-devices, the software does not use pythons libraries, such as PyUSB. This is because these softwares also have OS dependencies, that need to be installed and do not work that well in docker containers. USButils works great in contianers and is easy to install, which is why it has been chosen over pythons own libraries."
  },
  {
    "objectID": "Documentation/technologies.html#command-line-interface",
    "href": "Documentation/technologies.html#command-line-interface",
    "title": "Technological choices",
    "section": "Command-line interface",
    "text": "Command-line interface\nAn API client is automatically generated from the OpenAPI definition provided by FastAPI. The generation is done with OpenAPI Generator. A command line tool interfacing the autogenerated client is built with Typer.\n\nOpen API Generator\nOpenAPI Generator enables building extensive Python clients with documentation. Generating a client makes it easy to design customized workflows around the API. This project uses the client as the main component of the command-line interface. The autogeneration also builds templates for tests. More generally using a generator was a way to test automatic code generation.\n\n\nTyper\nTyper is an easy to use Python library for building command-line interfaces. Typer can be used to build light weight CLI’s so it’s a good fit for the autogenerated client. Typer also uses Python type hints and provides automatic help functions that include required arguments and a command’s description from docstrings."
  },
  {
    "objectID": "Documentation/use_cases.html",
    "href": "Documentation/use_cases.html",
    "title": "Use case scenarios",
    "section": "",
    "text": "As a security-conscious homeowner, I want to use TinyML(*) for human-detection to monitor my home and receive alerts when unexpected activity is detected, so that I can take action to ensure my home is safe.\n\nData Collection: User collects video footage from their smart camera, which will be used to train the TinyML model for human-detection.\nModel Training: User trains a TinyML human-detection model using the collected video footage and publicly available datasets.\nModel Squeezing: User optimizes the model size to ensure it can be deployed on TinyML-enabled devices.\nModel Deployment: User deploys the optimized TinyML model on their smart camera.\nInference: The smart camera uses the deployed TinyML model to perform real-time human-detection, identifying humans and activities within its field of view.\nAlerts: If the camera detects unusual activity, such as a person entering the home when no one is expected to be there, it sends alerts to the user’s smartphone.\nModel Update: User periodically updates the TinyML model with new data to ensure its accuracy and improve its performance over time.\nUser Access: User can access real-time video footage and receive notifications to confirm the alert and take appropriate action, such as contacting the police or checking in on the home from a remote location.\n\n(*) The alternative to TinyML would be running human detection in the cloud, with all the possible network latency issue we may face. TinyML-based human-detection can trigger an alarm installed “in place” where the camera is. By relying on cloud, we would need to transfer the video footage to the cloud and then running inference at there."
  },
  {
    "objectID": "Documentation/software-project-summer-kick-off.html",
    "href": "Documentation/software-project-summer-kick-off.html",
    "title": "Project starting point",
    "section": "",
    "text": "This was a successor of the previous software project in Winter. Here are the info:\n\nVideo Sprint3 demo, Mid\nVideo Sprint7 demo, Final\nSource code in GH Repo\nProject documentation, generated automatically\n\n\n\n\nimage.png\n\n\n     \n\nGoal for summer 2023 project\nRight now there’s no clear boundary between UI and its backend. We want to make them separted. The current Streamlit UI should be a pure frontend. The backend logic should be a REST backend server (e.g. fast API) Finall we want a CLI tool to control in additon to the current UI. A CLI tool should do the exact same things as the UI does right now.\n$ tmlaas device list\n&lt;list device name&gt;\n$ tmlaas model list\n&lt;list device name&gt;\n$ tmlass device=&lt;device id&gt; install model=&lt;model id&gt;\nYou may want to refer to this project as CLI example, https://ghapi.fast.ai/\n\n\nDevelopment environment\n\nSCRUM, User story mapping to set common goals with all stakeholders.\nNbdev, Jupyter notebook framework for code, (unit)tests & doc at once, Nbdev tutorial video.\nDocker compose to run the whole system at once, turorial video.\nAcceptance Test Driven Development (ATDD) to sync up with a client.\nStreamlit for UI framework used in this project.\nGH Project as Kanban\nGH Workflow as CI/CD\n\n\n\nCommunication\n\nDiscord, Click to join.\n\n\n\nNext\n\nWho’s SCRUM master for Sprint1?\nSprint1 (Week20)\n\nInitial research of this project by Students\n\nSprint1 review & planning at 10:00AM 22nd May\n\nReview WoW proposal from students\nQ&A for a client\nPrioritize user story?\n\nWhich Kanban board to share with customer?"
  },
  {
    "objectID": "Bridge/installing.html",
    "href": "Bridge/installing.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "The bridge uses Docker in Docker (dind) to spin up a container and using that to install the model on an Arduino.\n\nsource\n\nupload_rpi\n\n upload_rpi ()\n\nUploads compiled person detection uf2 file to device. The device must be in the USB Mass Storage Mode and device_path should be the absolute path at which the device is mounted at.\n\nsource\n\n\narduino_installer\n\n arduino_installer (device:dict, compiled_model:str)\n\nInstall the wanted model to a Arduino\n\nsource\n\n\ninstall_inference\n\n install_inference (device:dict, model:str)\n\nSelect the appropriate installer for the device and call that installer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "TinyMLaaS-main",
    "section": "Overview",
    "text": "Overview\nThe GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing."
  },
  {
    "objectID": "index.html#repositories",
    "href": "index.html#repositories",
    "title": "TinyMLaaS-main",
    "section": "Repositories",
    "text": "Repositories\n\nBackend\nFrontend\nCLI\nMCU components"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "TinyMLaaS-main",
    "section": "Documentation",
    "text": "Documentation\n\nWay of Working\nProduct backlog\nWorking hours\nDatabase schema"
  },
  {
    "objectID": "index.html#running-the-project",
    "href": "index.html#running-the-project",
    "title": "TinyMLaaS-main",
    "section": "Running the project",
    "text": "Running the project\nUse Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\n\nRunning individual parts of the project\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI"
  },
  {
    "objectID": "Bridge/observing.html",
    "href": "Bridge/observing.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "source\n\nread_prediction_from_port\n\n read_prediction_from_port (port:str)\n\nReads a single prediction line over serial from the MCU device and returns dict containing scores with keys Person and No person. Returns None if serial fails."
  },
  {
    "objectID": "Documentation/background.html",
    "href": "Documentation/background.html",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined."
  },
  {
    "objectID": "Documentation/background.html#federated-learning-in-tinyml",
    "href": "Documentation/background.html#federated-learning-in-tinyml",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined."
  },
  {
    "objectID": "Documentation/background.html#edge-impulse",
    "href": "Documentation/background.html#edge-impulse",
    "title": "Background information and some literature sources",
    "section": "Edge Impulse",
    "text": "Edge Impulse\nEdge Impulse is the leading development platform for ML on edge devices. It is free for developers. With Edge Impulse one can: - build advanced embedded ML apps - build custom datasets rapidly - collect sensor, audio or camera data from devices, files or cloud integrations - use automatic labeling tools such as object detection and audio segmentation - use Edge Impulse cloud infrastructure to set up and run reusable scripted operations that transform the input data on large sets of data in parallel - integrate deployment pipelines, CI/CD tools and custom data sources with open APIs - develop models and algortihms - with prebuilt DSP (Digital Signal Processors) and ML blocks - hardware decisions can be made on device performance and Flash/RAM on every step - DSP feature extraction algorithms can be customized - custom machine learning models with Keras APIs - use visualized insights on datasets, model performance and memory to fine-tune the production model - optimize models and algorithms - EON TUNER for finding balance between DSP configurations and model architecture, budgeted against memory and latency constraints - EON Compiler for lighter and faster neural networks with equal accuracy - have full visibility across the whole ML pipeline - complete access to data attributes, DSP algorithms, model hyperparameters throughout whole development lifecycle - test model performance accurately - virtual cloud hardware simulation framework to get performance and accuracy metrics before deploying on any physical device - model performance can be evaluated with live classification - devices, automated ML pipeline testing, integration with the testing framework - deploy easily on any edge target - optimize source code by generating optimized embedded libraries and applications for any edge device - build ready-to-go binaries with selected development boards supported by Edge Impulse with special firmware - without OS or hardware dependencies and compile to nearly anything - make digital twin by re-deploying cloud-hosted ML projects to any hardware target on the fly - benefit from access and integrations to the leading hardware partner ecosystem from MCUs to MPUs and GPUs including acceleration - Arduino - Himax - OpenMV - Nvidia - Nordic semiconductor - Raspberry Pi - Silicon Labs - Sony - ST - Syntiant - Texas Instruments"
  },
  {
    "objectID": "Documentation/background.html#arduino-and-wiring",
    "href": "Documentation/background.html#arduino-and-wiring",
    "title": "Background information and some literature sources",
    "section": "Arduino and Wiring",
    "text": "Arduino and Wiring\nArduino is an open-source electronics platform intended for anyone to make interactive projects. It is based on easy-to-use hardware and software. Arduino boards can - read inputs: light on a sensor, finger on a button, twitter message - turn inputs into outputs: activate motor, turn on a LED, publish something online - be instructed what to do by sending a set of instructions to the microcontroller on the board - using Arduino programming language (based on Wiring) and the Arduino Software (IDE) (based on Processing, an open source integrated development environment (IDE) like the Arduino IDE)\nBecause of simple and accessible user experience Arduino has been used in thousands of projects and applications by a worldwide community of makers, students, hobbyists, artists, programmers and professionals to produce a vast amount of accessible knowledge. The product range includes products for IoT applications, wearable, 3D printing and embedded environments.\nThe Arduino software - is easy-to-use for beginners but flexible enough for advanced users - runs on Mac, Windows and Linux - is used by teachers and students to build low cost scientific instruments to prove chemistry and physics principles and to get started with programming and robotics - is used by designers and architects to build interactive prototypes - is used by musicians and artists to build installations ansd to experiment with new musical instruments - is used by makers to build projects - can be used by anyone by following detailed instructions of a kit or sharing ideas online\nArduino offers some pros over other microcontrollers and microcontroller platforms available: - inexpensive compared to other microcontrolle platforms; the least expensive version of the Arduino module can be assembled by hand, pre-assembled Arduino modules are also affordable - cross-platform, the Arduino software (IDE) runs on Windows, Macintosh OSX, Linux (most microcontroller systems are limited to Windows) - simple, clear programming environment - easy-to-use for beginners but flexible enough for advanced users - because it’s based on Processing programming environment, students learning to program in that environment will be familiar with how the Arduino IDE works - open source and extensible software, published as open source tools - the language is based on AVR-C programming and can be expanded through C++ libraries - AVR-C code can be added directly into the Arduino programs - open source and extensible hardware, with the plans of the Arduino boards published under a Creative Commons licence, so own versions of the module with extensions and improvements can be built by anyone\nCode can be developed in the Arduino Cloud to build smart IoT projects. - smart devices can be connected within minutes - wide range of compatible devices, the Arduino Cloud provides the necessary code - nice dashbords can be created with mix and match customizable widgets to visualize real time or historical data or to control the device - projects can be controlled from anywhere in the world from any device, for example Alexa. - projects and libraries are always synced and up to date - all projects are cloud based and accessible from any device - data is always ecrypted and always belongs to the user (you) - is open and customizable, has flexible APIs to integrate and customize Cloud - all connected Arduino boards have a built-in crypto chip that makes them incredibly secure - sketches and project data are stored in AES 256-bit encrypted datastores - account security is protected with single use authentication codes - open and transparent data privacy terms and your data always belongs to you - has a wide range of resources - tutorials - APIs - documentation\nArduino’s history is interesting. The Arduino project was based on the developing platform Wiring created by Hernando Barragán as a Master’s thesis project at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy in 2003.\nHernando has been developing Wiring ever since and now Wiring is an open source electronics prototyping platform composed of programming language, an integrated development environment (IDE) , and a single-board microcontroller. More about Wiring can be found here.\nWiring offers new boards to customers in their webshop, but Wiring supports other boards directly, so Wiring IDE can be used on other boards as well (and more will be added). Thus Arduino board can be used with the Wiring IDE. Wiring can be downloaded for Linux, MacOS X and Windows."
  },
  {
    "objectID": "Documentation/background.html#mbed",
    "href": "Documentation/background.html#mbed",
    "title": "Background information and some literature sources",
    "section": "Mbed",
    "text": "Mbed\nMbed is an open-source platform and operating system for internet-connected devices that are based on 32-bit ARM Cortex-M microcontrollers. These devices are known as IoT devices. Mbed is collaboratively developed by Arm and its tech partners.\nMbed is free and open source IoT operating system with connectivity, security, storage, device management and ML. Mbed offers free development tools, thousands of code examples and support for hundreds of microcontrollers development boards, as described here. - Mbed has its own Mbed OS with - well-defined API to develop C++ applications - free tools and thousands of code examples, libraries and drivers for common components - built-in security stack - core components such as storage and connectivity options - Mbed Enabled hardware has many options - Compiler and IDE - Keil Studio Cloud - modern, zero-installation Mbed development environment in the browser - code high-lighting, WebUSB flash and debug and version control - Mbed Studio - an IDE for application and library development - single environment with everything to create, compile and debug Mbed programs - Mbed CLI - command line interface allows to integrate Mbed functionality into preferred editor or enhance automation setup - Security - Arm Mbed TLS provides comprehensive SSL/TLS solution - easy to include cryptographic and SSL/TLS capabilities in the software and embedded products - as an SSL library Arm Mbed TLS provides an intuitive API, readable source code and a minimal and highly configurable code footprint"
  },
  {
    "objectID": "Documentation/background.html#ifttt",
    "href": "Documentation/background.html#ifttt",
    "title": "Background information and some literature sources",
    "section": "IFTTT",
    "text": "IFTTT\nIFTTT is a private company that runs online digital automation platforms and offers them as a service. IFTTT is short for If This Then That. IFTTT integrates apps, devices and services quick and easy. IFTTT makes tech incompatibility easy to tackle. Automating process is simple, the user chooses - trigger - action(s) - name for the applet and finish\nIFTTT has over 700 services ready (more added weekly) to be automated. Price range of the services is from 0€/forever to 5€/month. IFTTT provides a simple way to create for example a smart home: - make a user account and log in (can be done with Google or Facebook) - trigger: give Google Assistant a voice command “Hey Google, I need coffee” - action(s): coffee machine is turned on and when the coffee is ready, the coffee machine turns off - name the applet: “make coffee” and finish the applet"
  },
  {
    "objectID": "Documentation/background.html#sony-mesh",
    "href": "Documentation/background.html#sony-mesh",
    "title": "Background information and some literature sources",
    "section": "Sony MESH",
    "text": "Sony MESH\nSony MESH is a Sony corporate startup that sells a range of colored blocks with different sensors and wireless connection to the IoT. It’s digital DIY platform to connect everyday objects into IoT and create your own projects. - MESH blocks are wireless - Visual Coding App called Canvas simplifies programming and wiring with drag-and-drop functions - project can be connected to web services and popular smart gadgets like WeMo and Google Assistant voice activation - hardware projects can be expanded without expertise - IoT block in the project allows additions of smart features such as motion-sensitivity, remote control, orientation monitoring, voice commands, notifications, text messaging and more - projects can be connected to the internet instantly - project can be transformed into an IoT device, such as - Twitter alarm system - a voice-activated, data-logging, remote-controlled car - allows customization of smart gadgets - MESH is compatible with over 350 smart gadgets, home automation devices and web services on IFTTT - each IoT block has built-in IFTTT integration, so that it’s simple to add custom features on a smart gadget - MESH Motion and MESH Temperature & Humidity used together allow addition of motion-activated, multi-room temperature monitoring to a smart device like Nest thermostat - allows to build own smart gadget - MESH GPIO is a simple interface for development boards like Arduino and Raspberry Pi or actuators like a DC motor - MESH GPIO integrates any smart devices or web services on IFTTT, incl. - Amazon Alexa for Echo - Google Assistant - Google Sheets - LIFX - Nest - Phillips Hue - Twitter - WeMo - over 350 more - MESH blocks use Bluetooth - MESH blocks are rechargeable, durable and compact"
  },
  {
    "objectID": "Documentation/background.html#sources",
    "href": "Documentation/background.html#sources",
    "title": "Background information and some literature sources",
    "section": "Sources:",
    "text": "Sources:\nOn-Device Training of Machine Learning Models on Microcontrollers with Federated Learning\nTinyFedTL: Federated Transfer Learning on Ubiquitous Tiny IoT Devices\nEdge Impulse\nBeginner’s Guide to DSP\nKeras APIs\nArduino\nArduino Getting started guide\nArduino Tutorials on Arduino Project Hub\nMbed\nMbed (product)\nIFTTT\nMESH blocks\nWiring\nWiring webshop"
  },
  {
    "objectID": "Documentation/architecture.html",
    "href": "Documentation/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "This page contains general information about the architechture and how each component is related to each other."
  },
  {
    "objectID": "Documentation/architecture.html#overview",
    "href": "Documentation/architecture.html#overview",
    "title": "Architecture",
    "section": "Overview",
    "text": "Overview\nTinyMLaaS consist of several components: - backend - streamlit frontend - cli - MCU components\nThe different components are stored in their own repositories which can be found here.\nThe backend is the core component which contains all the API endpoints. By calling them you can execute all the tasks necessary for the workflow. The backend is responsible with communicating with the machine learing components, storing data in the database and installing & managing MCU devices.\nTensorflow machine learning components live in the main repository and need to be fetched for the backend seperately.\nMCU repository contains the bridge for communicating with the devices and the code needed for devices.\nWe have implemented two different interfaces for the TinyMLaaS: CLI and website GUI using streamlit. Since you can make API calls directly to backend it’s extremely simple to build your own frontends in the future."
  },
  {
    "objectID": "Documentation/architecture.html#block-diagram",
    "href": "Documentation/architecture.html#block-diagram",
    "title": "Architecture",
    "section": "Block Diagram",
    "text": "Block Diagram\n\n\n\nBlock Diagram\n\n\nThe backend is the main component that deals with calling the tensorflow functions and communicating with the MCU devices. Tensorflow is currently the supported UI but you can also make API calls directly or use the CLI. In the future the tensorflow components can be containarized as their own service."
  },
  {
    "objectID": "Documentation/architecture.html#key-components",
    "href": "Documentation/architecture.html#key-components",
    "title": "Architecture",
    "section": "Key Components",
    "text": "Key Components\n\nML model training\nData Storage and loading (database)\nML model quantization and optimization\nML model compilation for MCUs"
  },
  {
    "objectID": "Documentation/architecture.html#database-diagram",
    "href": "Documentation/architecture.html#database-diagram",
    "title": "Architecture",
    "section": "Database diagram",
    "text": "Database diagram\n\n\n\nDatabase Diagram"
  },
  {
    "objectID": "Documentation/next_steps.html",
    "href": "Documentation/next_steps.html",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module."
  },
  {
    "objectID": "Documentation/next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "href": "Documentation/next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module."
  },
  {
    "objectID": "Documentation/next_steps.html#database",
    "href": "Documentation/next_steps.html#database",
    "title": "Suggestions for further development",
    "section": "Database",
    "text": "Database\n\nProduction ready database\nThe backend uses SQLite. However, as SQlite is so lightweight, there are drawbacks, that affect the usage of the software. First of all, SQLite is not meant for storing big files. Because of this, all datasets, models and compiled models are stored outside the database in directories, and the database contains the path to the files. This is not ideal, as the backend can get messy with all the directories and if permanent storage is required outside the docker container, all of these volumes need to be mounted to the docker container.\nA SQLite database is also a single file, meaning that accidentally deleting the database or misplacing it is more common.\nBecause of these drawbacks, we would suggest changing the used database from SQLite to something more robust and production ready, such as MariaDB or PostgreSQL. Larger files can be stored in these databases and it is easy to mount one database rather than multiple different locations for all different saving locations.\n\n\nMove saving of models and datasets to database\nAs mentioned in the previous section, all datasets and models are stored outside the database. With the change of the database, it would be better to save all these files in the database."
  },
  {
    "objectID": "Documentation/next_steps.html#datasets",
    "href": "Documentation/next_steps.html#datasets",
    "title": "Suggestions for further development",
    "section": "Datasets",
    "text": "Datasets\n\nDownloading/uploading datasets (as zip)\nCurrently, you can only upload and append to datasets. It’s not possible to view or download it anyway. The application should have way to download the entire dataset to your machine (propably as zip so it works on both Windows and Linux).\nAt the moment you can only send bulk of pictures of to the app to create a dataset. It’s propably good idea to allow sending zips and creating datasets from them. You propably need to make sure that - it’s a valid dataset with working images - the saved dataset keeps same folder structure as the send zip - the user doesn’t send malicious data\n\n\nEditing, deleting and managing datasets\nCurrently, only adding and appending datasets is supported. The user should propably be able to manage the datasets better. First you should be able to remove unwanted datasets. Second, the user should be able to make folder structure for the datasets and edit them for labeling purposes see this tutorial for example how the data is structured. Being able to simply download the datasets,editing it locally, removing it from the app and readding it allows for the user to edit it. After that, if you want to enhance the user experience one idea is to add better editing options to the app itself: Being able remove photos, being able to add photos, creating new folders, removing folders."
  },
  {
    "objectID": "Documentation/next_steps.html#cli",
    "href": "Documentation/next_steps.html#cli",
    "title": "Suggestions for further development",
    "section": "CLI",
    "text": "CLI\nThe CLI is mostly autogenerated with this OpenAPI Generator. The current generator works as far as the existing API endpoints stay the same regarding input and output. Whenever new HTTP methods are added or modified a new generation has to be done. This can be done by using the json-file provided by the FastAPI backend. See detailed instructions here.\n\nAdd missing functionality + fixes to existing\nOnly some of the functionality is available with the CLI. See examples from the already implemented functionality from services and tiny_mlaas.py. To help with implementing more functionality, see examples for using the client from the docs. For example for bridges here.\nAdding and listing existing devices isn’t working properly because newly added devices don’t have a designated bridge. It’s worth considering if adding a device should be possible without a bridge. This fix would be implemented in backend. Alternative fix is to modify the validation done by the generated client.\nTraining via the CLI trains a model, but the output contains a picture that can’t be displayed in CLI. A possible fix is to modify the training function so that displaying an example of a prediction is optional.\nMany of the functions have hardcoded parameters. This helps with development and testing, but should be fixed in the future. See example of adding parameters: https://github.com/TinyMLaas/TinyML-CLI/blob/main/services/models.py . Here dataset_id and description are required parameters. Epoch and the rest of the hard coded variables can be handled in a similar fashion.\n\n\nAutogenerate end-to-end CLI from OpenAPI YAML\nThe current version of the CLI is autogenerated with the exception of services and tiny_mlaas.py.\nEnd-to-end autogeneration could be done after publishing services and tiny-mlaas.py as a Python package and having it as a dependency. See instructions: https://typer.tiangolo.com/tutorial/package/.\nA preliminary idea for implementing the end-to-end generation is following:\n\nPublish a package that contains the forementioned files\nOpenAPI yaml-file is needed for generating the CLI tool. Get it by browsing to backend_url/openapi.json. Convert the json to yaml with for example: https://editor.swagger.io/\n\nThe repository for autogeneration would consist of the yaml -file from step 2 and requirements.txt -file with the package published in step 1 (in addition to current requirements). With these prerequisites the steps for end-to-end generation could be:\n\nClone repo\nInstall the generator tool with: npm install @openapitools/openapi-generator-cli -g\nGenerate the client with npx @openapitools/openapi-generator-cli generate -i file.yaml -g python -o output_path\n\nfile.yaml is the yaml-file in the repo\noutput path should probably be the root directory of the cloned CLI-repository\n\nInstall the requirements: pip install -r requirements.txt\nFinish the installation with: python3 setup.py install\n\nFor usage instructions see: https://github.com/TinyMLaas/TinyML-CLI#usage\nMisc: if requirements.txt is overwritten by the generator use a different name, interface_requirements.txt etc (remember to install these in step 4). Using the CLI package might differ from using it locally, see Typer documentation: https://typer.tiangolo.com/tutorial/package/"
  },
  {
    "objectID": "Documentation/next_steps.html#bridge",
    "href": "Documentation/next_steps.html#bridge",
    "title": "Suggestions for further development",
    "section": "Bridge",
    "text": "Bridge\n\nSupport for Raspberry PI\nInstallation and observation for Arduino nano 33 BLE has been imported fully to this version of the version. However, the Raspberry PI support has not been tested at all and most likely will not work out of the box. Adding support/making sure installation to Raspberries work is a required feature, that unfortunately does not exist yet.\n\n\nBridge port\nA bridge can be saved to the backend as an IP address and as an URL. There is right now no validation to make sure that the given IP address or URL is a valid address. Also, if IP address is used, it automatically asumes that the bridge is hosted on port 5000. This should be changed so that the bridge can be hosted on any port.\n\n\nError handling\nThere is little to none error handling on the bridge. This means that, even if operations fail, it will still say the operation was successfull. The next ones are known errors that do not have error handling:\n\nWhen the compiled arduino sketch is uploaded successfully, but it can not be started for some reason, most likely because there isn’t enough memory for model on the device.\nObservation doesn’t have permission to the device.\n\nThe installation process has a chance to fail when running inside a docker container and the reason is unknown."
  },
  {
    "objectID": "Tensorflow/training.html",
    "href": "Tensorflow/training.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This module takes a dataset in which the images are in labeled folders and trains a Keras neural network for image recognition.\n\nsource\n\nTrainModel\n\n TrainModel (data_dir)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTrainModel.load_data\n\n TrainModel.load_data (img_height, img_width, batch_size)\n\nLoads data from the directory provided in data_dir\n\nsource\n\n\nTrainModel.train\n\n TrainModel.train (img_height, img_width, epochs, optim_choice,\n                   batch_size, model_path)\n\nTrains a new neural network model.\nArgs: img_height (int): image pixel height img_width (int): image pixel width epochs (int): Number of epochs to train optim_choice (string): Loss function to be used\nReturns: keras_model, statistics\n\nsource\n\n\nTrainModel.prediction\n\n TrainModel.prediction (model, class_names:list)\n\nPredicts on the image provided in the path. Args: model (tflite model): tflite model to be used in the prediction\nReturns: img: image predicted, result: formatted string for the result\n\nsource\n\n\nTrainModel.plot_statistics\n\n TrainModel.plot_statistics (history, epochs_range)\n\nPlot model training statistics.\nArgs: history (tuple?): tuple containing loss and accuracy values over training epochs_range (int): amount of epochs used to train over\nReturns: BytesIO buffer: Matplotlib figure containing graphs about the training process\n\nsource\n\n\nTrainModel.continue_training\n\n TrainModel.continue_training (img_height, img_width, epochs, batch_size,\n                               model_path)\n\nTakes an excisting model and further trains it with more images.\nArgs: img_height (int): image pixel height img_width (int): image pixel width epochs (int): Number of epochs to train optim_choice (string): Loss function to be used batch_size (int): Batch size to be used model_path (string): Location for fetching the model\nReturns: keras_model, statistics"
  }
]