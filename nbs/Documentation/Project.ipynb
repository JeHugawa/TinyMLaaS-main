{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../docs/general\"\n",
    "\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_files = [f for f in files if \".md\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![GitHub Actions](https://github.com/Origami-TinyML/tflm_hello_world/workflows/CI/badge.svg)\n",
       "[![codecov](https://codecov.io/github/ArttuLe/tflm_hello_world/branch/main/graph/badge.svg?token=ZUUMIJNJ6C)](https://codecov.io/github/ArttuLe/tflm_hello_world)\n",
       "---\n",
       "title: \"Hello World of TensorFlow Lite for Microcontrollers\"\n",
       "---\n",
       "Run [Hello World](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world)\n",
       "example of [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level)\n",
       "in Docker automatically, hooked in CI (Github workflow).\n",
       "\n",
       "#  Sprint 1\n",
       "We are adding `Observation` (Prediction) UI in WebApp (TinyMLaaS) with [streamlist](https://streamlit.io/).\n",
       "\n",
       "0. `DevOps`, Install [nbdev](https://nbdev.fast.ai/) in this repo, to convert a Jupyter notebook to a .py file with unit tests.\n",
       "1. `DevOps`, Add unit test [code coverage](https://forums.fast.ai/t/nbdev-code-coverage-n-tests/73993) for `*.ipynb` files.\n",
       "   - Implement a failing unit test before implementing a feature.\n",
       "   - Confirm an unit test is failing, and implement a feature itself.\n",
       "2. `DevOps`, Add [acceptance test](https://blog.devgenius.io/testing-streamlit-a1f1fd48ce8f) (AT) with `TinyMLaaS.py`.\n",
       "   - Implement a failing AT for BI before implement an BI itself.\n",
       "   - Send a PR only for a failing AT at first, and we could agree on DoD (i.e. PR for a feature)\n",
       "4. Put a WebApp and a Hello respectively in a container within `docker-compose`\n",
       "5. A Hello container sends data to a WebApp container via simple TCP via [netcat](https://quickref.me/nc)\n",
       "7. Observe & display Hello prediction on a WebApp at real time\n",
       "9. `DevOps`, Dockerization, run by docker-compose\n",
       "10. `DevOps`, Hook a PR in CI / CD with Github workflow\n",
       "11. `DevOps`, Display the CI result on `Github page` via nbdev\n",
       "\n",
       "### DoD\n",
       "- A `PR merge` is DoD.\n",
       "- Covert a BI to GH issues (i.e tasks you think needed)\n",
       "- Each BI should have an acceptance criteria.\n",
       "- Each acceptance criteria should be implemented as an acceptance test in robot framework.\n",
       "- All acceptance test is triggered by any PR in CI.\n",
       "\n",
       "## File structure\n",
       "```mermaid\n",
       "  graph TD;\n",
       "      subgraph nbdev_work\n",
       "          n(nbdev) --add unit tests--> a & b & c;\n",
       "          a[training.ipynb]--convert automatically-->ap[training.py];\n",
       "          b[model.ipynb]--convert automatically-->bp[model.py];\n",
       "          c[observing.ipynb]--convert automatically-->cp[observing.py];\n",
       "      end\n",
       "      ap--import-->as[pages/4_Training.py];\n",
       "      bp--import-->bs[pages/3_Model.py];\n",
       "      cp--import-->cs[pages/7_Observing.py];\n",
       "      subgraph streamlit_work\n",
       "          Tr[TinyMLaaS.robot]--add acceptance tests-->Tp((TinyMLaaS.py));\n",
       "          as & bs & cs --a subpage of--> Tp;\n",
       "      end\n",
       "      Tp --Streamlit--> Published\n",
       "```\n",
       "  \n",
       "\n",
       "# Sprint 0\n",
       "You'll implement the following \"Test1\".\n",
       "You can skip training part for now. You need the following steps:\n",
       "\n",
       "1. git clone <TensorFlow repo>\n",
       "2. Follow hello world example instruction\n",
       "3. Run make with appropriate parameter for standalone x86[*](https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms)\n",
       "4. Build an executable binary with a model compiled in[*](https://github.com/ehirdoy/tflm)\n",
       "5. Run an executable binary on x86[*](https://www.tensorflow.org/lite/microcontrollers/library#build_binaries)[*](https://asciinema.org/a/552162)\n",
       "6. Put the above all in a container, reproducible with Docker file, inc all needed packages with \"apt install\" & \"git clone <TensorFlow repo>\" \n",
       "7. Add existing unit tests for this hello world[*](https://www.tensorflow.org/lite/microcontrollers/library#run_the_tests)\n",
       "8. Add test code coverage measurement infrastructure (-gcov in gcc?)\n",
       "9. Add a new acceptance test, How should we ensure this \"Hello world\"?\n",
       "10 Add some dashboard of coverage on github page(?) if no way to present the coverage[*](https://quarto.org/docs/publishing/github-pages.html)\n",
       "11. Hook a PR in CI / CD (github action / workflow) to run the above all\n",
       "\n",
       "![](tdd.png)\n",
       "\n",
       "Please feel free to edit this document to share information with others correctly.\n",
       "\n",
       "# Backlog?\n",
       "1. A WebApp (TinyMLaaS) installs a TinyML Hello model in a Hello container  \n",
       "2. Train \"Hello world\" with Jupyter notebook (JN) as instructed in TFLm website, recommended to all.\n",
       "3. Convert the above training JN to a WebApp with streamlit\n",
       "- Convert to a complete TinyML on a VM?  \n",
       "- Try other ML model?\n",
       "- Try real HW?\n",
       "- Try remote CI / CD?\n",
       "- Try better UI?\n",
       "\n",
       "   \n",
       "# User journey\n",
       "https://miro.com/app/board/uXjVPwQdIjc=/\n",
       "\n",
       "   \n",
       "# Slides\n",
       "- https://github.com/Origami-TinyML/software-engineering-project/blob/artifacts/kickoff.pdf\n",
       "- https://github.com/Origami-TinyML/software-engineering-project/blob/artifacts/soft_eng_proj_tinyml_lifecycle.md\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Technological choices\n",
       "\n",
       "This document is meant to answer why certain technological choices have been made and why certain frameworks have been chosen.\n",
       "\n",
       "## Main\n",
       "\n",
       "The `TinyMLaaS-main` -repository is meant for mostly building the whole software with [docker](https://www.docker.com/) and documentation. However, some development is also done in this repository. All tensorflow modules are coded here in [Jupyter notebooks](https://jupyter.org/).\n",
       "\n",
       "### Jupyter notebooks and NBDEV\n",
       "\n",
       "As jupyter notebooks can not be used as modules in python, they need to be exported into python modules. This is done with [NBDEV](https://nbdev.fast.ai/). Nbdev also automatically creates documentation from the jupyter notebooks and deploys them to [Github pages](https://tinymlaas.github.io/TinyMLaaS/)\n",
       "\n",
       "### Docker\n",
       "\n",
       "Running the software is meant to be done with *docker*. Docker allows running the software on different computers, without the software being platform spesific. Also, all the dependencies required for the software do not need to be installed on the host, rather, they will all be installed in the seperate *docker container*. If you are not familiar with docker, check out University of Helsinkis course [Devops with Docker](https://devopswithdocker.com/) materials to get a basic understanding of docker.\n",
       "\n",
       "## Frontend\n",
       "\n",
       "### Streamlit\n",
       "\n",
       "The frontend of the software is build with [Streamlit](https://streamlit.io/). This is done to make the development process faster, as this frontend is mainly meant for demo purposes. Streamlit makes it easy to create good looking websites, however, there isn't much room for cutomization and some features can be quite difficult to create.\n",
       "\n",
       "There is still a dependecy on `usbutils`. This will be talked more about in the [Bridge-section](#Bridge)\n",
       "\n",
       "## Backend\n",
       "\n",
       "The backend is the heart of the software. It does all the communication between all the other modules and does a lot of the heavy lifting of the software. It is created as a API.\n",
       "\n",
       "### FastAPI\n",
       "\n",
       "The backend is created with [FastAPI](https://fastapi.tiangolo.com/). FastAPI is very powerful for creating API:s, as it has great data validation with the help of [Pydantic](https://docs.pydantic.dev/latest/), it automatically creates good documentation about the different API requests and is simple to understand. To checkout more, read the [Starting documentation](https://fastapi.tiangolo.com/tutorial/). \n",
       "\n",
       "When deploying the API to production, the api will most likely be behind a proxy with some URL that has prefixes. For example, it might be deployed to *example.uri.com/api/*. For the API to function correctly, the root-path of the API needs to be declared for FastAPI, in this case, */api/*. \n",
       "\n",
       "### SQLAlchemy\n",
       "\n",
       "The backend talks with the database with sqlalchemy. This means that it is able to talk with any SQL-database without any changes to the backends software.\n",
       "\n",
       "As of now, the database in use is *sqlite*. However, this is meant to be more of a temporary solution to make development easier. For more information, checkout the suggestions in [Suggestions for further development](next_steps.md)\n",
       "\n",
       "## Bridge\n",
       "\n",
       "The relay is the part of the software to which the microcontrollers are connected to. It is also done in API style.\n",
       "\n",
       "### Flask\n",
       "\n",
       "Unlike the backend, the bridge is created with [Flask](https://flask.palletsprojects.com/en/2.3.x/). Flask is lightweight and easy to understand, which makes sence for the bridge, as the hardware, on which the bridge runs, might not be that powerfull.\n",
       "\n",
       "### Usbutils\n",
       "\n",
       "To find USB-devices, the software does not use pythons libraries, such as [PyUSB](https://github.com/pyusb/pyusb). This is because these softwares also have OS dependencies, that need to be installed and do not work that well in docker containers. *USButils* works great in contianers and is easy to install, which is why it has been chosen over pythons own libraries.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Architecture Guide\n",
       "This page contains general information about the architechture and how each component is related to each other.\n",
       "\n",
       "## overview\n",
       "\n",
       "TinyMLaaS consist of several components:\n",
       " - backend\n",
       " - streamlit frontend\n",
       " - cli\n",
       " - MCU components\n",
       "\n",
       "The different components are stored in their own repositories which can be found [here](https://github.com/orgs/TinyMLaas/repositories).\n",
       "\n",
       "The backend is the core component which contains all the API endpoints. By calling them you can execute all the tasks necessary for the workflow. The backend is responsible with communicating with the machine learing components, storing data in the database and installing & managing MCU devices.\n",
       "\n",
       "Tensorflow machine learning components live in the main repository and need to be fetched for the backend seperately.\n",
       "\n",
       "MCU repository contains the bridge for communicating with the devices and the code needed for devices.\n",
       "\n",
       "We have implemented two different interfaces for the TinyMLaaS: CLI and website GUI using streamlit. Since you can make API calls directly to backend it's extremely simple to build your own frontends in the future.\n",
       "\n",
       " \n",
       "## Block Diagram\n",
       "\n",
       "![Block Diagram](images/block_diagram.png)\n",
       "\n",
       "The backend is the main component that deals with calling the tensorflow functions and communicating with the MCU devices. Tensorflow is currently the supported UI but you can also make API calls directly or use the CLI. In the future the tensorflow components can be containarized as their own service.\n",
       "\n",
       "## Key Components\n",
       "- ML model training\n",
       "- Data Storage and loading (database)\n",
       "- ML model quantization and optimization\n",
       "- ML model compilation for MCUs\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Use case scenario to demonstrate TinyML as-a-Service\n",
       "\n",
       "As a security-conscious homeowner, I want to use TinyML(*) for human-detection to monitor my home and receive alerts when unexpected activity is detected, so that I can take action to ensure my home is safe.\n",
       "\n",
       "1. Data Collection: User collects video footage from their smart camera, which will be used to train the TinyML model for human-detection.\n",
       "2. Model Training: User trains a TinyML human-detection model using the collected video footage and publicly available datasets.\n",
       "3. Model Squeezing: User optimizes the model size to ensure it can be deployed on TinyML-enabled devices.\n",
       "4. Model Deployment: User deploys the optimized TinyML model on their smart camera.\n",
       "5. Inference: The smart camera uses the deployed TinyML model to perform real-time human-detection, identifying humans and activities within its field of view.\n",
       "6. Alerts: If the camera detects unusual activity, such as a person entering the home when no one is expected to be there, it sends alerts to the user's smartphone.\n",
       "7. Model Update: User periodically updates the TinyML model with new data to ensure its accuracy and improve its performance over time.\n",
       "8. User Access: User can access real-time video footage and receive notifications to confirm the alert and take appropriate action, such as contacting the police or checking in on the home from a remote location.\n",
       "\n",
       "(*) The alternative to TinyML would be running human detection in the cloud, with all the possible network latency issue we may face. TinyML-based human-detection can trigger an alarm installed \"in place\" where the camera is. By relying on cloud, we would need to transfer the video footage to the cloud and then running inference at there."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Architecture Guide\n",
       "\n",
       "This page contains general information about the applications architecture and how different parts of the application work and communicate with each other.\n",
       "\n",
       "## Key Components\n",
       "- ML model training \n",
       "- Data storage & loading\n",
       "- ML model quantization and optimization\n",
       "- ML model compilation\n",
       "- Firmware packaging\n",
       "\n",
       "## Enabling Technologies\n",
       "- Tensorflow;\n",
       "- Docker\n",
       "- Streamlit\n",
       "- Aws S3\n",
       "\n",
       "## What Has Been Implemented?\n",
       "\n",
       "![General flow of the service](images/asd.png)\n",
       "\n",
       "- Right now uploading the firmware is supported via uploading the firmware to Dockerhub and notifying the Bridging server to pull the docker image and upload the firmware via Serial on to the device.\n",
       "- TODO regarding the above picture containing all the elements of the application is the ability to update Over-The-Air for Wifi-enabled devices.\n",
       "\n",
       "### Implemented Features\n",
       "- ML model training\n",
       "- Data storage to LocalStack/S3\n",
       "- ML model optimization and compression\n",
       "- Supported devices Arduino Nano 33 BLE Sense and RPI pico\n",
       "- Querying prediction results from the end-device\n",
       "- Refer to the actual application and the above picture for more information.\n",
       "\n",
       "## Neural Network Architecture\n",
       "\n",
       "![Current NN Architecture](images/model.tflite.png)\n",
       "\n",
       "When training a Neural Network for TinyML, there are a few things to keep in mind to ease the process. The resulting model must fit into the device.\n",
       "\n",
       "Things that affect the resulting models size include:\n",
       "- Amount of layers used in the model. Fewer layers usually results in smaller sized models.\n",
       "- Used OPs, e.g., Convolution layers can take up quite a bit of space.\n",
       "- Quantization is almost necessary for the model to compressed into a usable size.\n",
       "\n",
       "TinyML related tips\n",
       "- OPs used in the keras model training must also be supported in tflite-micro.\n",
       "- [Netron](netron.app), Use netron to visualize models and check that you have all the OPs enabled in your Firmware. Every OP must be declared in order for the model to work.\n",
       "\n",
       "Misc.\n",
       "- [Tensorflow](https://www.tensorflow.org/tutorials/images/classification) has extensive amount of information and tutorials regarding almost all necessary things when it comes to the application's ML related stuff.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Background information and some literature sources\n",
       "\n",
       "## Federated learning in TinyML\n",
       "\n",
       "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. [TinyFedTL](https://ieeexplore.ieee.org/abstract/document/9767250) is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\n",
       "\n",
       "While recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. [On-device training](https://www.mdpi.com/2079-9292/11/4/573) has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.\n",
       "\n",
       "## Edge Impulse\n",
       "\n",
       "[Edge Impulse](https://www.edgeimpulse.com/) is the leading development platform for ML on edge devices. It is free for developers. With Edge Impulse one can:\n",
       "- build advanced embedded ML apps\n",
       "- build custom datasets rapidly\n",
       "    - collect sensor, audio or camera data from devices, files or cloud integrations\n",
       "    - use automatic labeling tools such as object detection and audio segmentation\n",
       "    - use Edge Impulse cloud infrastructure to set up and run reusable scripted operations that transform the input data on large sets of data in parallel\n",
       "    - integrate deployment pipelines, CI/CD tools and custom data sources with open APIs\n",
       "- develop models and algortihms\n",
       "    - with prebuilt DSP [(Digital Signal Processors)](https://www.analog.com/en/design-center/landing-pages/001/beginners-guide-to-dsp.html) and ML blocks\n",
       "    - hardware decisions can be made on device performance and Flash/RAM on every step\n",
       "    - DSP feature extraction algorithms can be customized\n",
       "    - custom machine learning models with [Keras APIs](https://keras.io/)\n",
       "    - use visualized insights on datasets, model performance and memory to fine-tune the production model\n",
       "- optimize models and algorithms\n",
       "    - [EON TUNER](https://www.edgeimpulse.com/blog/introducing-the-eon-tuner-edge-impulses-new-automl-tool-for-embedded-machine-learning) for finding balance between DSP configurations and model architecture, budgeted against memory and latency constraints\n",
       "    - [EON Compiler](https://www.edgeimpulse.com/blog/introducing-eon) for lighter and faster neural networks with equal accuracy\n",
       "- have full visibility across the whole ML pipeline\n",
       "    - complete access to data attributes, DSP algorithms, model hyperparameters throughout whole development lifecycle \n",
       "- test model performance accurately\n",
       "    - virtual cloud hardware simulation framework to get performance and accuracy metrics before deploying on any physical device\n",
       "    - model performance can be evaluated with live classification\n",
       "        - devices, automated ML pipeline testing, integration with the testing framework\n",
       "- deploy easily on any edge target\n",
       "    - optimize source code by generating optimized embedded libraries and applications for any edge device\n",
       "    - build ready-to-go binaries with selected development boards supported by Edge Impulse with special firmware\n",
       "    - without OS or hardware dependencies and compile to nearly anything\n",
       "    - make digital twin by re-deploying cloud-hosted ML projects to any hardware target on the fly\n",
       "- benefit from access and integrations to the leading hardware partner ecosystem from MCUs to MPUs and GPUs including acceleration\n",
       "    - Arduino\n",
       "    - Himax\n",
       "    - OpenMV\n",
       "    - Nvidia\n",
       "    - Nordic semiconductor\n",
       "    - Raspberry Pi\n",
       "    - Silicon Labs\n",
       "    - Sony\n",
       "    - ST\n",
       "    - Syntiant\n",
       "    - Texas Instruments\n",
       "\n",
       "## Arduino and Wiring\n",
       "\n",
       "[Arduino](https://www.arduino.cc/) is an open-source electronics platform intended for anyone to make interactive projects. It is based on easy-to-use hardware and software. [Arduino boards can](https://www.arduino.cc/en/Guide/Introduction)\n",
       "- read inputs: light on a sensor, finger on a button, twitter message\n",
       "- turn inputs into outputs: activate motor, turn on a LED, publish something online\n",
       "- be instructed what to do by sending a set of instructions to the microcontroller on the board\n",
       "    - using Arduino programming language (based on [Wiring](https://en.wikipedia.org/wiki/Wiring_%28development_platform%29)) and the Arduino Software (IDE)  (based on [Processing](https://www.arduino.cc/education/visualization-with-arduino-and-processing), an open source integrated development environment (IDE) like the Arduino IDE)\n",
       "\n",
       "Because of simple and accessible user experience Arduino has been used in thousands of projects and applications by a worldwide community of makers, students, hobbyists, artists, programmers and professionals to produce a vast amount of accessible knowledge. The product range includes products for IoT applications, wearable, 3D printing and embedded environments.\n",
       "\n",
       "The Arduino software\n",
       "- is easy-to-use for beginners but flexible enough for advanced users\n",
       "- runs on Mac, Windows and Linux\n",
       "- is used by teachers and students to build low cost scientific instruments to prove chemistry and physics principles and to get started with programming and robotics\n",
       "- is used by designers and architects to build interactive prototypes\n",
       "- is used by musicians and artists to build installations ansd to experiment with new musical instruments\n",
       "- is used by makers to build projects\n",
       "- can be used by anyone by following detailed instructions of a kit or sharing ideas online\n",
       "\n",
       "Arduino offers some pros over other microcontrollers and microcontroller platforms available:\n",
       "- inexpensive compared to other microcontrolle platforms; the least expensive version of the Arduino module can be assembled by hand, pre-assembled Arduino modules are also affordable\n",
       "- cross-platform, the Arduino software (IDE) runs on Windows, Macintosh OSX, Linux (most microcontroller systems are limited to Windows)\n",
       "- simple, clear programming environment\n",
       "    - easy-to-use for beginners but flexible enough for advanced users\n",
       "    - because it's based on Processing programming environment, students learning to program in that environment will be familiar with how the Arduino IDE works\n",
       "- open source and extensible software, published as open source tools\n",
       "    - the language is based on AVR-C programming and can be expanded through C++ libraries\n",
       "    - AVR-C code can be added directly into the Arduino programs \n",
       "- open source and extensible hardware, with the plans of the Arduino boards published under a Creative Commons licence, so own versions of the module with extensions and improvements can be built by anyone\n",
       "\n",
       "Code can be developed in the [Arduino Cloud](https://cloud.arduino.cc/) to build smart IoT projects.\n",
       "- smart devices can be connected within minutes\n",
       "    - wide range of compatible devices, the Arduino Cloud provides the necessary code\n",
       "- nice dashbords can be created with mix and match customizable widgets to visualize real time or historical data or to control the device\n",
       "- projects can be controlled from anywhere in the world from any device, for example Alexa.\n",
       "- projects and libraries are always synced and up to date\n",
       "- all projects are cloud based and accessible from any device\n",
       "- data is always ecrypted and always belongs to the user (you)\n",
       "- is open and customizable, has flexible APIs to integrate and customize Cloud\n",
       "- all connected Arduino boards have a built-in crypto chip that makes them incredibly secure\n",
       "- sketches and project data are stored in AES 256-bit encrypted datastores\n",
       "- account security is protected with single use authentication codes\n",
       "- open and transparent data privacy terms and your data always belongs to you\n",
       "- has a wide range of resources\n",
       "    - [tutorials](https://docs.arduino.cc/cloud/iot-cloud)\n",
       "    - [APIs](https://docs.arduino.cc/arduino-cloud/getting-started/api-overview)\n",
       "    - [documentation](https://docs.arduino.cc/cloud/iot-cloud/tutorials/technical-reference)\n",
       "\n",
       "Arduino's [history](https://en.wikipedia.org/wiki/Arduino#History) is interesting. The Arduino project was based on the developing platform Wiring created by Hernando Barragán as a Master's thesis project at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy in 2003.\n",
       "\n",
       "Hernando has been developing Wiring ever since and now Wiring is an open source electronics prototyping platform composed of programming language, an integrated development environment (IDE) , and a single-board microcontroller. More about Wiring can be found [here](http://wiring.org.co/). \n",
       "\n",
       "Wiring offers new boards to customers in their [webshop](https://eu.robotshop.com/), but Wiring supports other boards directly, so Wiring IDE can be used on other boards as well (and more will be added). Thus Arduino board can be used with the Wiring IDE. Wiring can be [downloaded](http://wiring.org.co/download/) for Linux, MacOS X and Windows.\n",
       "\n",
       "\n",
       "## Mbed\n",
       "\n",
       "[Mbed](https://en.wikipedia.org/wiki/Mbed) is an open-source platform and operating system for internet-connected devices that are based on 32-bit [ARM Cortex-M microcontrollers](https://www.arm.com/products/silicon-ip-cpu?families=cortex-m&showall=true). These devices are known as IoT devices. Mbed is collaboratively developed by [Arm](https://en.wikipedia.org/wiki/Arm_(company)) and its tech partners.\n",
       "\n",
       "Mbed is free and open source IoT operating system with connectivity, security, storage, device management and ML. Mbed offers free development tools, thousands of code examples and support for hundreds of microcontrollers development boards, as described [here](https://os.mbed.com/).\n",
       "- Mbed has its own Mbed OS with \n",
       "    - well-defined API to develop C++ applications\n",
       "    - free tools and thousands of code examples, libraries and drivers for common components\n",
       "    - built-in security stack\n",
       "    - core components such as storage and connectivity options\n",
       "- Mbed Enabled hardware has many options\n",
       "- Compiler and IDE\n",
       "    - Keil Studio Cloud\n",
       "        - modern, zero-installation Mbed development environment in the browser\n",
       "        - code high-lighting, WebUSB flash and debug and version control\n",
       "    - Mbed Studio\n",
       "        - an IDE for application and library development\n",
       "        - single environment with everything to create, compile and debug Mbed programs\n",
       "    - Mbed CLI\n",
       "        - command line interface allows to integrate Mbed functionality into preferred editor or enhance automation setup\n",
       "- Security\n",
       "    - Arm Mbed TLS provides comprehensive SSL/TLS solution\n",
       "    - easy to include cryptographic and SSL/TLS capabilities in the software and embedded products\n",
       "    - as an SSL library Arm Mbed TLS provides an intuitive API, readable source code and a minimal and highly configurable code footprint\n",
       " \n",
       "## IFTTT\n",
       "\n",
       "[IFTTT](https://ifttt.com/) is a private company that runs online digital automation platforms and offers them as a service. IFTTT is short for If This Then That. IFTTT integrates apps, devices and services quick and easy. IFTTT makes tech [incompatibility easy to tackle](https://ifttt.com/explore/new_to_ifttt). Automating process is simple, the user chooses\n",
       "- trigger\n",
       "- action(s)\n",
       "- name for the applet and finish\n",
       "\n",
       "IFTTT has over 700 services ready (more added weekly) to be automated. Price range of the services is from 0€/forever to 5€/month. IFTTT provides a simple way to create for example a smart home:\n",
       "- make a user account and log in (can be done with Google or Facebook)\n",
       "- trigger: give Google Assistant a voice command \"Hey Google, I need coffee\"\n",
       "- action(s): coffee machine is turned on and when the coffee is ready, the coffee machine turns off\n",
       "- name the applet: \"make coffee\" and finish the applet\n",
       "\n",
       "## Sony MESH\n",
       "\n",
       "[Sony MESH](https://meshprj.com/en/) is a Sony corporate startup that sells a range of colored blocks with different sensors and wireless connection to the IoT. It's digital DIY platform to connect everyday objects into IoT and [create your own projects](https://www.bundl.com/articles/examples-sony-launch-and-crowdfund-iot-corporate-startup).\n",
       "- MESH blocks are wireless\n",
       "- Visual Coding App called Canvas simplifies programming and wiring with drag-and-drop functions\n",
       "- project can be connected to web services and popular smart gadgets like WeMo and Google Assistant voice activation\n",
       "- hardware projects can be expanded without expertise\n",
       "    - IoT block in the project allows additions of smart features such as motion-sensitivity, remote control, orientation monitoring, voice commands, notifications, text messaging and more\n",
       "- projects can be connected to the internet instantly\n",
       "    - project can be transformed into an IoT device, such as \n",
       "        - Twitter alarm system\n",
       "        - a voice-activated, data-logging, remote-controlled car\n",
       "- allows customization of smart gadgets\n",
       "    - MESH is compatible with over 350 smart gadgets, home automation devices and web services on IFTTT\n",
       "    - each IoT block has built-in IFTTT integration, so that it's simple to add custom features on a smart gadget\n",
       "        - MESH Motion and MESH Temperature & Humidity used together allow addition of motion-activated, multi-room temperature monitoring to a smart device like Nest thermostat\n",
       "- allows to build own smart gadget\n",
       "    - MESH GPIO is a simple interface for development boards like Arduino and Raspberry Pi or actuators like a DC motor\n",
       "    - MESH GPIO integrates any smart devices or web services on IFTTT, incl.\n",
       "        - Amazon Alexa for Echo\n",
       "        - Google Assistant\n",
       "        - Google Sheets\n",
       "        - LIFX\n",
       "        - Nest\n",
       "        - Phillips Hue\n",
       "        - Twitter\n",
       "        - WeMo\n",
       "        - over 350 more\n",
       "- MESH blocks use Bluetooth\n",
       "- MESH blocks are rechargeable, durable and compact\n",
       "\n",
       "## Sources:\n",
       "\n",
       "[On-Device Training of Machine Learning Models on Microcontrollers with Federated Learning](https://www.mdpi.com/2079-9292/11/4/573)\n",
       "\n",
       "[TinyFedTL: Federated Transfer Learning on Ubiquitous Tiny IoT Devices](https://ieeexplore.ieee.org/abstract/document/9767250)\n",
       "\n",
       "[Edge Impulse](https://www.edgeimpulse.com/)\n",
       "\n",
       "[Beginner's Guide to DSP](https://www.analog.com/en/design-center/landing-pages/001/beginners-guide-to-dsp.html)\n",
       "\n",
       "[Keras APIs](https://keras.io/)\n",
       "\n",
       "[Arduino](https://www.arduino.cc/)\n",
       "\n",
       "[Arduino Getting started guide](https://www.arduino.cc/en/Guide)\n",
       "\n",
       "[Arduino Tutorials on Arduino Project Hub](https://projecthub.arduino.cc/)\n",
       "\n",
       "[Mbed](https://www.arm.com/products/development-tools/embedded-and-software/mbed-os)\n",
       "\n",
       "[Mbed (product)](https://os.mbed.com/)\n",
       "\n",
       "[IFTTT](https://ifttt.com/)\n",
       "\n",
       "[MESH blocks](https://meshprj.com/en/)\n",
       "\n",
       "[Wiring](http://wiring.org.co/)\n",
       "\n",
       "[Wiring webshop](https://eu.robotshop.com/)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Demonstratation of TinyMLaaS WebApp\n",
       "\n",
       "This document will demonstrate the steps in TinyMLaaS app. Some pages have dependencies on other pages, so go through pages from top to bottom.\n",
       "\n",
       "In order to run TinyMLaaS end-to-end following items needs to be run simultanously:\n",
       "- Expose port 5000 for the bridging device to work with outside sources. (command: ngrok http 5000)\n",
       "- Start up the bridging server. (command: python3 relay/main.py)\n",
       "- Launch the app (command: make app)\n",
       "\n",
       "Add the environment variables DOCKERHUB_USER and DOCKERHUB_PASSWORD in docker-compose.yml for Dockerhub (don't commit these changes).\n",
       "\n",
       "## Device\n",
       "\n",
       "On device page, user should open register a bridging device expander and add an IP address of the bridging server. \n",
       "\n",
       "![Ngrok bridging server](images/device_ready.png)\n",
       "\n",
       "Use the link from ngrok and wait until the app will confirm that a registration of the bridging server was successfull.\n",
       "\n",
       "![IP address registrated](images/device_done.png)\n",
       "\n",
       "If IP address is not provided on this section, devices cannot be utilized in the later tabs.\n",
       "\n",
       "## Data\n",
       "\n",
       "First, select a dataset used for training of the model. \n",
       "\n",
       "![Dataset upload finalized](images/dataset_ready.png)\n",
       "\n",
       "Extracting images from archived file with localstack should take about 1-3min. This time should be reduced greatly when the datasets are actually stored in S3. After the app has confirmed the dataset has been uploaded, user is able to view images included the dataset. \n",
       "\n",
       "User can add images from local storage to selected dataset and label those. If the uploaded images are unlabeled they are not used for training in later stage of the process. However user can label those later if needed. \n",
       "\n",
       "Note that if you upload many images, streamlit's performance might reduce. Also, images added to dataset is available only for this session, the app doesn't support saving new images to datasets permanently. \n",
       "\n",
       "## Model\n",
       "\n",
       "For now the app supports only one model and therefore, this tab has been filled with dummy data. Changing selections on this tab doesn't impact to the model. \n",
       "\n",
       "To continue press Select button.\n",
       "\n",
       "## Training\n",
       "\n",
       "Before training the Keras model, model training settings need to be defined. A user need to enter information regarding number of epocs, batch size, image width and image height. The current model can be only trained if image width and image height are same and if loss function is Sparse Categorical.\n",
       "\n",
       "Here are example inputs for training a model.\n",
       "\n",
       "![Training input settings](images/training_settings.png)\n",
       "\n",
       "Press Train to continue.\n",
       "\n",
       "## Compiling\n",
       "\n",
       "The page is responsible for ML compilation. This page is also filled with dummy data, so selections won't impact on compiling result. \n",
       "\n",
       "Press Compile to continue.\n",
       "\n",
       "## Installing\n",
       "\n",
       "On the installing sheet, user must select a model and a device. Like stated before, app supports one model and two devices.\n",
       "\n",
       "The user must have plugged the device into the USB-port before building the docker image. It is good to check that the device is plugged into correct port, since plugging into incorrect port creates an error. If build time seems to be unexpectedly long, it might be good idea to check there is no 'No device found on ttyACM0' warning when running the relay server. That means the commands used to communicate with a device require sudo rights. This can be easily fixed by altering device commands in the relay/main.py module.\n",
       "\n",
       "![Real-time predictions as device output](images/sudo.png)\n",
       "\n",
       "The building time varies depending on the device. The Arduino nano 33 BLE takes about 10-15min and Pico 5-10min to install. After building, a model is ready to be sent to the device.\n",
       "\n",
       "\n",
       "![Real-time predictions as device output](images/installation.png)\n",
       "\n",
       "To continue, user must first press Generate button (builds the docker image and upload it to dockerhub) and when that has been finalized user press Install button (installs the model on to the device using the docker image downloaded from dockerhub). If user wants to change the device, whole page needs to be rerun with new device.\n",
       "\n",
       "## Observing\n",
       "\n",
       "On the observing page, user can see real-time predictions from device when the start button has been activated. A plugged device sends approximately every second a prediction. Notice that with current model and camera the predictions are not very accurate. User can end printing new predictions to the app by pressing stop button.\n",
       "\n",
       "![Real-time predictions as device output](images/observing.png)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Suggestions for further development\n",
       "\n",
       "### Work in progress: Isolate tensorflow from backend\n",
       "\n",
       "The TinyMLaaS and TinyML-backend repositories both contain a branch called ```dockerize_tensorflow```. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running. \n",
       "\n",
       "Compiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress. \n",
       "\n",
       "The feature works with Docker in Docker (dind) using the [nestybox/sysbox runtime environment](https://github.com/nestybox/sysbox) which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\n",
       "\n",
       "The ```dockerize_tensorflow``` branch contains a [Dockerfile](https://github.com/TinyMLaas/TinyMLaaS/blob/dockerize_tensorflow/backend_no_tf.Dockerfile) that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS ```dockerize_tensorflow``` branch uses backend_no_tf.Dockerfile to pull the ```dockerize_tensorflow``` branch from the TinyML-backend repo.\n",
       "\n",
       "Our FastAPI backend communicates with the docker image and containers using [Docker API](https://docs.docker.com/engine/api/) and more specifically the [Docker SDK for Python](https://docker-py.readthedocs.io/en/stable/index.html). The feature is implemented in the [tf_docker/compile.py](https://github.com/TinyMLaas/TinyML-backend/blob/dockerize_tensorflow/tf_docker/compile.py) backend module.\n",
       "\n",
       "## Database\n",
       "\n",
       "### Production ready database\n",
       "\n",
       "The backend uses [SQLite](https://www.sqlite.org/index.html). However, as SQlite is so lightweight, there are drawbacks, that affect the usage of the software. First of all, SQLite is not meant for storing big files. Because of this, all datasets, models and compiled models are stored **outside** the database in directories, and the database contains the **path** to the files. This is not ideal, as the backend can get messy with all the directories and if permanent storage is required outside the docker container, all of these volumes need to be mounted to the docker container.\n",
       "\n",
       "A SQLite database is also a single file, meaning that accidentally deleting the database or misplacing it is more common.\n",
       "\n",
       "Because of these drawbacks, we would suggest changing the used database from SQLite to something more robust and production ready, such as [MariaDB](https://mariadb.org/) or [PostgreSQL](https://www.postgresql.org/). Larger files can be stored in these databases and it is easy to mount one database rather than multiple different locations for all different saving locations.\n",
       "\n",
       "### Move saving of models and datasets to database\n",
       "\n",
       "As mentioned in the previous [section](#Change database for more production ready database), all datasets and models are stored outside the database. With the change of the database, it would be better to save all these files in the database.\n",
       "\n",
       "\n",
       "## Datasets\n",
       "\n",
       "### Downloading/uploading datasets (as zip)\n",
       "Currently, you can only upload and append to datasets. It's not possible to view or download it anyway. The application should have way to download the entire dataset to your machine (propably as zip so it works on both Windows and Linux).\n",
       "\n",
       "At the moment you can only send bulk of pictures of to the app to create a dataset. It's propably good idea to allow sending zips and creating datasets from them. You propably need to make sure that \n",
       "- it's a valid dataset with working images\n",
       "- the saved dataset keeps same folder structure as the send zip\n",
       "- the user doesn't send malicious data\n",
       "\n",
       "### Editing, deleting and managing datasets\n",
       "\n",
       "Currently, only adding and appending datasets is supported. The user should propably be able to manage the datasets better. First you should be able to remove unwanted datasets. Second, the user should be able to make folder structure for the datasets and edit them for labeling purposes [see this tutorial for example how the data is structured](https://www.tensorflow.org/tutorials/images/classification#download_and_explore_the_dataset).\n",
       "Being able to simply download the datasets,editing it locally, removing it from the app and readding it allows for the user to edit it. After that, if you want to enhance the user experience one idea is to add better editing options to the app itself: Being able remove photos, being able to add photos, creating new folders, removing folders.\n",
       "\n",
       "## CLI\n",
       "\n",
       "The CLI is mostly autogenerated with [this](https://openapi-generator.tech/) OpenAPI Generator.\n",
       "\n",
       "### Add missing functionality + fixes to existing \n",
       "\n",
       "Only some of the functionality is available with the CLI. See examples from the already implemented functionality from [services](https://github.com/TinyMLaas/TinyML-CLI/tree/main/services) and [tiny_mlaas.py](https://github.com/TinyMLaas/TinyML-CLI/blob/main/tinyml_cli.py). To help with implementing the more, see examples for using the client from the docs. For example for bridges [here](https://github.com/TinyMLaas/TinyML-CLI/blob/main/docs/apis/tags/BridgesApi.md).\n",
       "\n",
       "Adding and listing existing devices isn't working properly because newly added devices don't have a designated bridge. It's worth considering if adding a device should be possible without a bridge. This fix would be implemented in backend. Alternative fix is to modify the validation done by the generated client.\n",
       "\n",
       "Many of the functions have hardcoded parameters. This helps with development and testing, but should be fixed in the future. See example of adding parameters: https://github.com/TinyMLaas/TinyML-CLI/blob/main/services/models.py . Here dataset_id and description are required parameters. Epoch and the rest of the hard coded variables can be handled in a similar fashion.\n",
       "\n",
       "### Autogenerate end-to-end CLI from OpenAPI YAML\n",
       "\n",
       "The current version of the [CLI](https://github.com/TinyMLaas/TinyML-CLI) is autogenerated with the exception of [services](https://github.com/TinyMLaas/TinyML-CLI/tree/main/services) and [tiny_mlaas.py](https://github.com/TinyMLaas/TinyML-CLI/blob/main/tinyml_cli.py).\n",
       "\n",
       "End-to-end autogeneration could be done after publishing services and tiny-mlaas.py as a Python package and having it as a dependency. See instructions: https://typer.tiangolo.com/tutorial/package/.\n",
       "\n",
       "A preliminary idea for implementing the end-to-end generation is following:\n",
       "\n",
       "1. Publish a package that contains the forementioned files\n",
       "\n",
       "2. OpenAPI yaml-file is needed for generating the CLI tool. Get it by browsing to backend_url/openapi.json. Convert the json to yaml with for example: https://editor.swagger.io/\n",
       "\n",
       "The repository for autogeneration would consist of the yaml -file from step 2 and requirements.txt -file with the package published in step 1 (in addition to current requirements). With these prerequisites the steps for end-to-end generation could be:\n",
       "\n",
       "1. Clone repo\n",
       "\n",
       "2. Install the generator tool with: npm install @openapitools/openapi-generator-cli -g\n",
       "\n",
       "3. Generate the client with npx @openapitools/openapi-generator-cli generate -i file.yaml -g python -o *output_path*\n",
       "     - file.yaml is the yaml-file in the repo\n",
       "     - output path should probably be the root directory of the cloned CLI-repository\n",
       "\n",
       "4. Install the requirements: pip install -r requirements.txt\n",
       "\n",
       "5. Finish the installation with: python3 setup.py install\n",
       "\n",
       "For usage instructions see: https://github.com/TinyMLaas/TinyML-CLI#usage\n",
       "\n",
       "Misc: if requirements.txt is overwritten by the generator use a different name, interface_requirements.txt etc (remember to install these in step 4). Using the CLI package might differ from using it locally, see Typer documentation: https://typer.tiangolo.com/tutorial/package/\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in md_files:\n",
    "    display(Markdown(f\"{path}/{f}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
