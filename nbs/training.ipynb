{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cc1a2-0b26-4c81-aa7e-d248ab4aabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64609f2d-2501-4650-a36e-089e26dc8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de073317-7d03-4b02-accb-e989b3426b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "\n",
    "    def load_data(self, img_height, img_width, batch_size):\n",
    "        \"\"\"\n",
    "        Loads data from the directory provided in data_dir\n",
    "        \"\"\"\n",
    "\n",
    "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            validation_split=0.2,\n",
    "            subset=\"training\",\n",
    "            seed=123,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            color_mode=\"grayscale\",)\n",
    "\n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=123,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            color_mode=\"grayscale\",)\n",
    "\n",
    "\n",
    "        return train_ds, val_ds\n",
    "\n",
    "\n",
    "    def train(self, img_height, img_width, epochs, optim_choice, batch_size,model_name):\n",
    "        \"\"\"Model training\n",
    "        \n",
    "        Args:\n",
    "            `img_height` (_int_): image pixel height\n",
    "            `img_width` (_int_): image pixel width\n",
    "            `epochs` (_int_): Number of epochs to train\n",
    "            `optim_choice` (_string_): Loss function to be used\n",
    "\n",
    "        Returns:\n",
    "            keras_model, statistics\n",
    "        \"\"\"\n",
    "        train_ds, validation_ds = self.load_data(img_height, img_width, batch_size)\n",
    "        class_names = train_ds.class_names\n",
    "\n",
    "\n",
    "        #Enable caching for training\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        train = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        test = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "        # Build model\n",
    "        model = Sequential([\n",
    "            layers.Reshape(target_shape=(img_width, img_height, 1), input_shape=(img_width, img_height)),\n",
    "            layers.experimental.preprocessing.Rescaling(1./255),\n",
    "            layers.Conv2D(16, 3, activation='relu', padding='SAME',),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.DepthwiseConv2D(8, 3, activation='relu', padding='SAME'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(units=2, activation='softmax'),\n",
    "          ])\n",
    "\n",
    "        if optim_choice == \"Categorical crossentropy\":\n",
    "            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        elif optim_choice == \"Sparse Categorical crossentropy\":\n",
    "            loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=loss_fn,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(\n",
    "          train,\n",
    "          validation_data=test,\n",
    "          epochs=epochs\n",
    "        )\n",
    "\n",
    "        epochs_range = range(epochs)\n",
    "\n",
    "        #temporary model saving\n",
    "        model.save(f'models/{model_name}',overwrite=True)\n",
    "        \n",
    "        return model, history, epochs_range\n",
    "\n",
    "\n",
    "    def prediction(self, model, class_names: list):\n",
    "        \"\"\"Predicts on the image provided in the path.\n",
    "        Args:\n",
    "            `model` (tflite model): tflite model to be used in the prediction\n",
    "\n",
    "        Returns:\n",
    "            img: image predicted, result: formatted string for the result\n",
    "        \"\"\"\n",
    "\n",
    "        path = f'{self.data_dir}{class_names[0]}/1.png'\n",
    "        model_shape = model.layers[0].input_shape\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (model_shape[1],model_shape[2]))\n",
    "\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "        names = {}\n",
    "\n",
    "        for index, value in enumerate(class_names):\n",
    "            names[index] = value\n",
    "\n",
    "        result = (\"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(names[np.argmax(score)], 100 * np.max(score)))\n",
    "\n",
    "        return img, result\n",
    "\n",
    "    def plot_statistics(self, history, epochs_range):\n",
    "        \"\"\"Plot model training statistics\n",
    "\n",
    "        Args:\n",
    "            `history` (tuple?): tuple containing loss and accuracy values over training\n",
    "            `epochs_range` (int): amount of epochs used to train over\n",
    "\n",
    "        Returns:\n",
    "            BytesIO buffer: Matplotlib figure containing graphs about the training process\n",
    "        \"\"\"\n",
    "\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        stats = plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss, label='Training Loss')\n",
    "        plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Training and Validation Loss')\n",
    "\n",
    "        buff = BytesIO()\n",
    "        stats.savefig(buff, format=\"png\")\n",
    "\n",
    "        return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad65604-0017-4942-b89e-2a3fae6bccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac1d1b-526e-4f22-bccb-08b4bf6feddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TrainModel.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcb47b-ea13-4a86-9ad3-36a441513308",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TrainModel.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec630ae4-51d2-4d04-973d-9ef081405731",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TrainModel.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c0dd5-782d-40ef-bfbf-db325f4ac5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TrainModel.plot_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec80a4-6cd5-4706-abf5-628a335ad160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
